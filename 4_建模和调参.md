- # 建模和调参

- 

- 参考优秀的学习资料

- 

- 逻辑回归模型：

  - 理解逻辑回归模型；
  - 逻辑回归模型的应用；
  - 逻辑回归的优缺点；

- 树模型：

  - 理解树模型；
  - 树模型的应用；
  - 树模型的优缺点；

- 集成模型

  - 基于bagging思想的集成模型
    - 随机森林模型
  - 基于boosting思想的集成模型
    - XGBoost模型
    - LightGBM模型
    - CatBoost模型

- 模型对比与性能评估：

  - 回归模型/树模型/集成模型；
  - 模型评估方法；
  - 模型评价结果；

- 模型调参：

  - 贪心调参方法；
  - 网格调参方法；
  - 贝叶斯调参方法；







### 4.4.1 逻辑回归

- 优点
  - 训练速度较快，分类的时候，计算量仅仅只和特征的数目相关；
  - 简单易理解，模型的可解释性非常好，从特征的权重可以看到不同的特征对最后结果的影响；
  - 适合二分类问题，不需要缩放输入特征；
  - 内存资源占用小，只需要存储各个维度的特征值；
- 缺点
  - **逻辑回归需要预先处理缺失值和异常值【可参考task3特征工程】；**
  - 不能用Logistic回归去解决非线性问题，因为Logistic的决策面是线性的；
  - 对多重共线性数据较为敏感，且很难处理数据不平衡的问题；
  - 准确率并不是很高，因为形式非常简单，很难去拟合数据的真实分布；

### 4.4.2 决策树模型

- 优点
  - 简单直观，生成的决策树可以可视化展示
  - **数据不需要预处理，不需要归一化，不需要处理缺失数据**
  - 既可以处理离散值，也可以处理连续值
- 缺点
  - 决策树算法非常容易过拟合，导致泛化能力不强（可进行适当的剪枝）
  - 采用的是贪心算法，容易得到局部最优解

### 4.4.3 集成模型集成方法（ensemble method）

通过组合多个学习器来完成学习任务，通过集成方法，可以将多个弱学习器组合成一个强分类器，因此集成学习的泛化能力一般比单一分类器要好。

集成方法主要包括Bagging和Boosting，Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个更加强大的分类。两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果。常见的基于Baggin思想的集成模型有：随机森林、基于Boosting思想的集成模型有：Adaboost、GBDT、XgBoost、LightGBM等。

**Baggin和Boosting的区别总结如下：**

- **样本选择上：** Bagging方法的训练集是从原始集中有放回的选取，所以从原始集中选出的各轮训练集之间是独立的；而Boosting方法需要每一轮的训练集不变，只是训练集中每个样本在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整
- **样例权重上：** Bagging方法使用均匀取样，所以每个样本的权重相等；而Boosting方法根据错误率不断调整样本的权值，错误率越大则权重越大
- **预测函数上：** Bagging方法中所有预测函数的权重相等；而Boosting方法中每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重
- **并行计算上：** Bagging方法中各个预测函数可以并行生成；而Boosting方法各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。



### 4.4.4 模型评估方法

## 建模与调参 

**线性回归**

**模型建立**

先使用线性回归来查看一下用线性回归模型来拟合我们的题目会有那些缺点。这里使用了 sklearn 的 LinearRegression。

- 

```
sklearn.linear_model.LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1
```

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpsxNDM71plgNFURiaxcHCfSSB98lsSfn29AYAy8KlHPqvBWY7ibU3InryQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 
- 
- 
- 

```
model = LinearRegression(normalize=True)model.fit(data_x, data_y)
model.intercept_, model.coef_
查看训练的线性回归模型的截距与权重

```

- 
- 
- 

```
'intercept:'+ str(model.intercept_)sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)## output
对上下文代码涉及到的函数功能进行简单介绍：
```

| 函数                              | 功能                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| zip()                             | 可以将两个可迭代的对象,组合返回成一个元组数据                |
| dict()                            | 元组数据构建字典                                             |
| items()                           | 以列表返回可遍历的(键, 值) 元组数组                          |
| sort(iterable, cmp, key, reverse) | 排序函数                                                     |
| iterable                          | 指定要排序的list或者iterable                                 |
| key                               | 指定取待排序元素的哪一项进行排序 - 这里x[1]表示按照列表中第二个元素排序 |
| reverse                           | 是一个bool变量，表示升序还是降序排列，默认为False(升序)      |
| np.quantile(train_y, 0.9)         | 求train_y 的90%的分位数                                      |



下面这个代码是把价格大于90%分位数的部分截断了,就是长尾分布截断



绘制特征v_9的值与标签的散点图，图片发现模型的预测结果（蓝色点）与真实标签（黑色点）的分布差异较大。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpsRdLPv38fneGpd6I59YGIO2piciaq6HYooYh9bpgiaOydk7eu8qicOCoX1A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

且预测值price出现负数，查看price分布 出现长尾分布 不符合正态分布。



**线性回归解决方案**

\1. 进行log变化

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpshwQpm4V6jC3qlgCSoqzgognxhMhsBebxZaIBZ7icEXic0H87B7VPIymg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

\2. 进行可视化，发现预测结果与真实值较为接近，且未出现异常状况。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpsPPaqAzZiccLesdMt7RQTurUKlib2yoTXJj4NVgibXW0wCib97rfEI6jWug/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**交叉****验证**

大概说一下sklearn的交叉验证的使用方法， 下文会有很多使用：

| verbose     | 日志显示                     |
| ----------- | ---------------------------- |
| verbose = 0 | 为不在标准输出流输出日志信息 |
| verbose = 1 | 为输出进度条记录             |
| verbose = 2 | 为每个epoch输出一行记录      |

K折交叉验证是将原始数据分成K组，将每个子集数据分别做一次验证集，其余的K-1组子集数据作为训练集，这样会得到K个模型，用这K个模型最终的验证集分类准确率的平均数，作为此K折交叉验证下分类器的性能指标。此处，采用五折交叉验证。

- 
- 
- 
- 
- 
- 

```
data_y = np.log(data_y + 1)# 交叉验证scores = cross_val_score(LinearRegression(normalize=True), X=data_x, \                        y=data_y, cv=5, scoring=make_scorer(mean_absolute_error))
np.mean(scores)
```

但在事实上，由于我们并不具有预知未来的能力，五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况。

通过2018年的二手车价格预测2017年的二手车价格，这显然是不合理的，因此我们还可以采用时间顺序对数据集进行分隔。

在本例中，我们选用靠前时间的4/5样本当作训练集，靠后时间的1/5当作验证集，最终结果与五折交叉验证差距不大。

- 

```
import datetimesample_feature = sample_feature.reset_index(drop=True)split_point = len(sample_feature) // 5 * 4train = sample_feature.loc[:split_point].dropna()val = sample_feature.loc[split_point:].dropna()
train_X = train[continuous_feature_names]train_y_ln = np.log(train['price'] + 1)val_X = val[continuous_feature_names]val_y_ln = np.log(val['price'] + 1)
model = model.fit(train_X, train_y_ln)
```

- `fill_between()`
- `train_sizes - 第一个参数表示覆盖的区域`
- `train_scores_mean - train_scores_std - 第二个参数表示覆盖的下限`
- `train_scores_mean + train_scores_std - 第三个参数表示覆盖的上限`
- `color - 表示覆盖区域的颜色`
- `alpha - 覆盖区域的透明度,越大越不透明 [0,1]`

预测结果查看：

- 

```
mean_absolute_error(val_y_ln, model.predict(val_X))
0.19443858353490887
```

**可视化处理**

绘制学习率曲线与验证曲线

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpseI7A0zA1eBic71HSuXEpBL9nRTTricI166IJia9tmEiaFZNVZJL3PABfqQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**线性模型**

先来对比一下三个lr模型的情况:

- 

```
models = [LinearRegression(),          Ridge(),          Lasso()]result = dict()for model in models:    model_name = str(model).split('(')[0]    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))    result[model_name] = scores    print(model_name + ' is finished')
result = pd.DataFrame(result)result.index = ['cv' + str(x) for x in range(1, 6)]result
```

结果对比如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53Mxpsbdbv0xIZ7FCKhxtZ9jEfDz4SPPpbPxEsBPcnspiceSuDUbEL9qE8u1g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

LinearRegression线性回归：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpsuLwjUJUx8Ba724u4ialHkjfbGa80Ku30ZePBojZEEpJkRICP8ibkSuCQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Lasso回归：L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。由此发现power与userd_time特征非常重要。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpsPIK07r7TqF0ZJTzLLVDBGG5ahvZWVibpurux51ZFGcJmUDEy7bAtBicA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Ridge回归：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpszqJcYxAAWbHSpibeReUFSMUlORPuv6avWb08y6ZX9mLiaNMkafFdwFqg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

L2正则化在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型，因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。

**非线****性模型**

```
**SVM**
通过寻求结构化风险最小来提高学习机泛化能力,基本模型定义为特征空间上的间隔最大的线性分类器支持向量机的学习策略便是间隔最大化。
```

\`````**SVR**：用于标签连续值的回归问题`

```
**SVC**：用于分类标签的分类问题
**Boosting**
```

`一堆弱分类器的组合就可以成为一个强分类器；不断地在错误中学习，迭代来降低犯错概率通过一系列的迭代来优化分类结果，每迭代一次引入一个弱分类器，来克服现在已经存在的弱分类器组合的短板。```

```
**Adaboost**
```

`整个训练集上维护一个分布权值向量W，用赋予权重的训练集通过弱分类算法产生分类假设（基学习器）y(x)， 然后计算错误率,用得到的错误率去更新分布权值向量w，对错误分类的样本分配更大的权值,正确分类的样本赋予更小的权值，每次更新后用相同的弱分类算法产生新的分类假设,这些分类假设的序列构成多分类器，对这些多分类器用加权的方法进行联合,最后得到决策结果`````

```
**Gradient Boosting**
```

`迭代的时候选择梯度下降的方向来保证最后的结果最好。损失函数用来描述模型的'靠谱'程度,假设模型没有过拟合,损失函数越大,模型的错误率越高。如果我们的模型能够让损失函数持续的下降,最好的方式就是让损失函数在其梯度方向下降。```

\```**GradientBoostingRegressor()**`

- `loss - 选择损失函数，默认值为ls(least squres),即最小二乘法,对函数拟合`
- `learning_rate - 学习率`
- `n_estimators - 弱学习器的数目,默认值100`
- `max_depth - 每一个学习器的最大深度,限制回归树的节点数目,默认为3`
- `min_samples_split - 可以划分为内部节点的最小样本数,默认为2`
- `min_samples_leaf - 叶节点所需的最小样本数,默认为1`

```
**MLPRegressor()**参数详解
```

- `hidden_layer_sizes - hidden_layer_sizes=(50, 50),表示有两层隐藏层，第一层隐藏层有50个神经元,第二层也有50个神经元`

- `activation - 激活函数   {‘identity’, ‘logistic’, ‘tanh’, ‘relu’},默认relu`

- `identity - f(x) = x`

- `logistic - 其实就是sigmod函数,f(x) = 1 / (1 + exp(-x))`

- `tanh - f(x) = tanh(x)`

- `relu - f(x) = max(0, x)`

- `solver - 用来优化权重     {‘lbfgs’, ‘sgd’, ‘adam’},默认adam,```

- `lbfgs - quasi-Newton方法的优化器:对小数据集来说,lbfgs收敛更快效果也``更好`

- `sgd - 随机梯度下降`

- `adam - 机遇随机梯度的优化器`

- `alpha - 正则化项参数,可选的，默认0.0001`

- `learning_rate - 学习率,用于权重更新,只有当solver为’sgd’时使用`

- `max_iter - 最大迭代次数,默认200`

- `shuffle - 判断是否在每次迭代时对样本进行清洗,默认True,只有当solver=’sgd’或者‘adam’时使用`

  

```
**XGBRegressor**``梯度提升回归树,也叫梯度提升机
```

- `采用连续的方式构造树,每棵树都试图纠正前一棵树的错误`
- `与随机森林不同,梯度提升回归树没有使用随机化,而是用到了强预剪枝`
- `从而使得梯度提升树往往深度很小,这样模型占用的内存少,预测的速度也快`

- 
- 

```
from sklearn.linear_model import LinearRegressionfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.ensemble import RandomForestRegressorfrom sklearn.ensemble import GradientBoostingRegressorfrom sklearn.neural_network import MLPRegressorfrom xgboost.sklearn import XGBRegressorfrom lightgbm.sklearn import LGBMRegressor
models = [LinearRegression(),          DecisionTreeRegressor(),          RandomForestRegressor(),          GradientBoostingRegressor(),          MLPRegressor(solver='lbfgs', max_iter=100),          XGBRegressor(n_estimators = 100, objective='reg:squarederror'),          LGBMRegressor(n_estimators = 100)]
result = dict()for model in models:    model_name = str(model).split('(')[0]    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))    result[model_name] = scores    print(model_name + ' is finished')
result = pd.DataFrame(result)result.index = ['cv' + str(x) for x in range(1, 6)]result
```

各模型结果：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53MxpsQQosj4AC1uNicHB2wFrTrhIETUQ2N8PF2mhmljKSic1XBTJR3f4UKLWA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

```
虽然随机森林模型在此时取得较好的效果，但LGB的效果与其相差不大。对LGB进行调参后结果会得到提高，下面对LGB进行简介。
**LightGBM**``使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低。思想是将连续的浮点特征离散成k个离散值，并构造宽度为k的Histogram。然后遍历训练数据，统计每个离散值在直方图中的累计统计量。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点。``````LightGBM采用leaf-wise生长策略：每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。
```

\```Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。`

```
参数:num_leaves - 控制了叶节点的数目，它是控制树模型复杂度的主要参数,取值应 <= 2 ^（max_depth）
bagging_fraction - 每次迭代时用的数据比例,用于加快训练速度和减小过拟合
feature_fraction - 每次迭代时用的特征比例,例如为0.8时，意味着在每次迭代中随机选择80％的参数来建树，boosting为random forest时用
min_data_in_leaf - 每个叶节点的最少样本数量。它是处理leaf-wise树的过拟合的重要参数。将它设为较大的值，可以避免生成一个过深的树。但是也可能导致欠拟合
max_depth - 控制了树的最大深度,该参数可以显式的限制树的深度
n_estimators - 分多少颗决策树(总共迭代的次数)
objective - 问题类型
regression - 回归任务,使用L2损失函数
regression_l1 - 回归任务,使用L1损失函数
huber - 回归任务,使用huber损失函数fair - 回归任务,使用fair损失函数
mape (mean_absolute_precentage_error) - 回归任务,使用MAPE损失函数
```

**模型调参**

```
常用的三种调参方法：贪心调参GridSearchCV调参贝叶斯调参
```

这里给出一个模型可调参数及范围选取的参考：

**贪心****调参**

拿当前对模型影响最大的参数调优，直到最优化；再拿下一个影响最大的参数调优，如此下去，直到所有的参数调整完毕。这个方法的缺点就是可能会调到局部最优而不是全局最优，但是省时间省力，巨大的优势面前，可以一试。

- 
- 

```
objectives = ["rank:map", "reg:gamma", "count:poisson", "reg:tweedie", "reg:squaredlogerror"]max_depths = [1, 3, 5, 10, 15]lambdas = [.1, 1, 2, 3, 4]

best_obj = dict()for obj in objective:    model = LGBMRegressor(objective=obj)    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))    best_obj[obj] = score    best_leaves = dict()for leaves in num_leaves:    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))    best_leaves[leaves] = score    best_depth = dict()for depth in max_depth:    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],                          max_depth=depth)    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))    best_depth[depth] = score
```

这里 “count:poisson” 的损失最小， 所以下个参数调试时会加上这个参数

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsG9H9sQ4u2liaG5YSH53Mxps0rVmdAAUQLCic2ZxzcBKwx6KuLzvcntD0ZMMPxeD2EhXKAg6ESucbkg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**GridSearchCV调参**

GridSearchCV，它存在的意义就是自动调参，只要把参数输进去，就能给出最优化的结果和参数。但是这个方法适合于小数据集，一旦数据的量级上去了，很难得出结果。这个在这里面优势不大， 因为数据集很大，不太能跑出结果，但是也整理一下，有时候还是很好用的。

- 

```
parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth}model = LGBMRegressor()clf = GridSearchCV(model, parameters, cv=5)clf = clf.fit(train_X, train_y)
clf.best_params_
model = LGBMRegressor(objective='regression',                          num_leaves=55,                          max_depth=15)
np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
0.13626164479243302
```

**贝叶斯调参**

贝叶斯优化用于机器学习调参，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。

与常规的网格搜索或者随机搜索的区别是：

```
贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部最优
```

使用方法：

```
定义优化函数(rf_cv, 在里面把优化的参数传入，然后建立模型， 返回要优化的分数指标)定义优化参数开始优化（最大化分数还是最小化分数等）得到优化结果
```



时间关系，模型太难啦

建议多看看多练练，









